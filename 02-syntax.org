#+TITLE: Programmiersprachen und Übersetzer
#+SUBTITLE: 02 - Syntaktische Analyse
#+INCLUDE: export-prologue.org
#+PROPERTY: header-args:latex :tangle yes :noweb yes :tangle-macros yes :tangle-prologue topic :tangle-epilogue endtopic :exports none

#+NAME: topic
#+BEGIN_SRC latex :tangle no
\psuSectionStart{{{{property(ITEM)}}}}{{{{n(block)}}}}
#+END_SRC

#+NAME: endtopic
#+BEGIN_SRC latex :tangle no
\psuSectionStop{{{{property(ITEM)}}}}{{{{n(block,-)}}}}
#+END_SRC

* Gedanken zu dieser Vorlesung                                     :noexport:
Mit der zweiten Vorlesung, will ich die Syntaktische Analyse
vollständig abdecken um möglichst schnell die Syntax als Vorhang vor
der eigentlichen Bedeutung der Sprache hinter uns zu lassen. Am Anfang
der Vorlesung soll der Zeichenstrom stehen und am Ende der Abstrakte
Syntaxbaum. Dabei will ich vermeiden jede erdenkliche Art und Weise
des Parsens zu behandeln, und nur schlaglichtartig einen Blick auf das
Thema der Parser zu richten.

- Motivation
  - ?
  - Sprachen sind eigentlich nicht Kontextfrei
- Lexer
  - Zerteilung in einen Tokenstrom
  - Kombination aus endlichen Automaten
- Parser
  - Wiederholung Kontextfreie Grammatiken
    - Backus-Naur-Form
  - Ableitungsbaum bzw. Parsebaum
  - Recursive Descent Parser/Syntaxgraphen/LL(1)
  - Vom Parsebaum zum Abstrakten Syntaxbaum
  - LR(1) Grammatiken und Parsergeneratoren (kurz)
  - C ist nur mit Symbol Table parsebar
  - Parsing ist ganz weites Feld, in dem es viele Unterschiedliche Klassen und Parsetechniken gibt. Was Menschen meist verwenden ist LALR(1).
- Regular or Context-Free!
  - Man wird ganz leicht eine weird machine.
- Zusammenfassung

Was lernt man aus dieser Vorlesung für die Ziele effektiv und effizient?
 - Syntax ist nur ein Vorhang vor der eigentlichen Semantik.
 - Der Studierende muss irgendwann den AST sehen und nicht mehr die Zeichen.
 - Man kommt ganz schnell in größere Komplexitätsklassen.

* Was ist die Syntax einer Sprache?
:PROPERTIES:
:CUSTOM_ID: 02-syntax-analyse
:END:

#+begin_src latex
\subtitle{{{{subtitle()}}}}
\begin{frame}
  \maketitle
\end{frame}
  \begin{frame}[t]{Einordnung in die Vorlesung: Syntaktische Analyse}
    \begin{center}
      \includegraphics[page=4,width=0.8\linewidth]{fig/01-overview-small}
    \end{center}
    \bi
    \ii Die Syntaktische Analyse ist der erste Schritt in einem Übersetzer.\medskip
    \ii Was sollte der \emph{effektive} und \emph{effiziente} Informatiker darüber Wissen? {
    \bi
    \ii Die Syntax ist die nur die Schreibweise, wie man die Konzepte notiert.
    \ii Rekursiv geschachtelte Elemente und Bäume passen zueinander.
    \ii Es gibt gute Formalismen und mächtige Werkzeuge zur Syntaktischen Analyse.
    \ei
    }
    \ei
  \end{frame}

  \begin{frame}{Was ist das Ziel der Syntaktischen Analyse?}
    \begin{center}
      \includegraphics[page=1,width=0.6\linewidth]{fig/01-overview-example}
    \end{center}
    \bi
    \ii Überprüfung der Syntaxregeln und Extraktion der Programmstruktur {
      \bi
      \ii Syntaktische Korrektheit ist \alert{ein} Teil der Sprachregeln.
      \ii Übersetzer für Programmiersprachen müssen Zeichenketten verarbeiten.
      \ii Im Weiteren benötigen wir die Programmstruktur als abstrakten Syntaxbaum.
      \ei
    }
    \ei
  \end{frame}
#+end_src

Ein Übersetzer für
(Maschinen)programme{{{see(01-ebenenmodell,Ebenenmodell)}}} wird
meistens mit einer, oft sehr langen, Zeichenkette konfrontiert, von
der der Programmierer behauptet es wäre ein valides Programm. Da der
Programmier beim kodieren wahrscheinlich unter Zeitdruck stand, faul
war, oder einfach ein anderes Verständnis von Ästhethik hat, darf der
Übersetzer kreative Formatierungen und allerhöchstens die Erfüllung
der minimalen Sprachregeln erwarten. Er muss also heraus finden, ob
diese Zeichenkette ein valides Programm der erwarteten Sprache
darstellt oder ob es fehlerhaft ist.

Der erste Schritt dieser akribischen Untersuchung des Quellprogramms
ist die *Syntaktische Analyse* oder *Syntaxanalyse*. Diese ist
Vergleichbar mit der Rechtschreib- und Grammatikprüfung bei
normalsprachlichen Texten, bei er es zunächst auch nur darum geht ob
alle Worte richtig geschrieben und die Regeln der Satzstruktur
eingehalten wurde. Dabei überprüft die Syntaxanalyse ebensowenig den
Sinn eines Satzes, welchen man auch als seine *Semantik* bezeichnen
kann, wie dies ein Rechtschreibprogramm tut.

Ein Beispiel, im Kontext von Programmiersprachen, für den Unterschied
zwischen Syntax und Semantik ist das folgende Python Programm. Es
stellt zwar ein syntaktisch korrektes Programm dar, ist also nach den
Regeln der Sprache aufgeschrieben worden, aber es macht ergibt keinen
Sinn, weil die Variable ~undef_variable~ nicht definiert wurde.
Versuchen sie im interaktiven Interpreter, der auch eine Syntaxanalyse
durchführt wie ein Übersetzer, einen Syntaxfehler zu provozieren.

#+begin_src python
foo = undef_variable + 3
#+end_src

Um zu Prüfen ob ein gegebenes Programm wirklich zum Maschinenmodell
passt, wird es demnach nicht ausreichen seine Syntaktische Korrektheit
zu prüfen. Aber es ist ein erster, wichtiger Schritt.

Als Seiteneffekt der Syntaktischen Analyse bekommen wir auch noch den
*abstrakten Syntaxbaum* oder AST (engl. abstract syntax tree). Dieser
Baum, der das Rechenergebnis von Scanner und Parser ist, fängt die
Struktur des Programms ein und wird in der folgenden Übersetzung als
Datenbasis benötigt. Mit dieser Struktur des Programms meinen wir so
etwas wie die korrekte Klammerung von arithmetischen Ausdrücken
(~1+2*3-4~ wird verstanden als ~1+(2*3)-4~) oder welche Codezeile zum
then-Teil einer Bedingung gehört.

Für den Informatiker ist die Beschäftigung mit dem Thema Scannen und
Parsen aus mehreren Gründen sehr lohnend: Durch den direkten Vergleich
von Programmtext und AST sieht man, dass die konkrete Syntax einer
Programmiersprache relativ egal ist, hauptsächlich mit Ästhetik,
Klarheit und leichter Verständlichkeit zusammehängt, und vom
Übersetzer bereits im ersten Schritt weg gewischt wird. Dennoch ist
die Extraktion von Strukturen aus einer Zeichenkette ein immer wieder
kehrendes Thema der Informatik, die nicht nur bei Programmiersprachen
auftritt, sondern, zum Beispiel, auch bei Dateiformaten oder Netzwerkprotokollen.

Daher hat die Informatik auch eine Reihe von präzisen Formalismen
entwickelt (Reguläre und Kontextfreie Sprachen) mit Hilfe derer man
solche Problem erfassen und angehen kann. Mit dieser Formalisierung
kam auch die Möglichkeit Werkzeuge zu bauen, die einem dabei helfen
Werkzeuge zur Syntaxanalyse zu erstellen
(Parsergeneratoren)[fn::Parsergeneratoren werden auch
Compiler-Compiler genannt, da sie Übersetzer sind, die Code erzeugen,
der in Übersetzern verwendet wird.].

* Scanner und Tokenstrom
:PROPERTIES:
:CUSTOM_ID: 02-scanner
:END:

#+begin_src latex
  \dividerframe{Scanner\\und\\Tokenstrom}

  \begin{frame}<handout:3>{Tokenstrom}
    \animation[width=\linewidth]{1/1,2/2-}{fig/02-token-stream}
    \bi
    \ii<2-> Gruppierung von Zeichen zu sprachspezifischen Bausteinen (Token) {
      \bi
      \ii Token haben eine \emph{Klasse} (func) und ggf. eine \emph{Nutzlast} (\texttt{"fnord"}, 23).
      \ii Tokenstrom enthält keine Leerzeichen oder Kommentare.
      \ii Unterschiedliche Schreibweisen (0x17, 23, 027, 0b10111) sind vereinheitlicht.
      \ii Gemeinsame Prefixe werden zum längsten Match aufgelößt (\texttt{fn} vs. \texttt{fnord})
      \ei
    }\medskip
    \ii<3-> Ziele der Abstraktion vom einzelnen Zeichen zum Token{
      \bi
      \ii \alert{Komplexitätsreduktion:} Es gibt immer weniger Token als Zeichen.
      \ii \alert{Fokusierung} auf die Sprachelemente, nicht auf ihre Schreibweise.
      \ii \alert{Vereinfachung} des Parsens durch weniger Sonderfälle (Kommentare).
      \ei
    }
    \ei
  \end{frame}

  \begin{frame}[t]{Scanner (oder Lexer)}
    \signature{scanner\_next}{Stream[char]}{Token}
    \bi
    \ii<2-> Der Scanner (auch Abtaster oder Lexer) erzeugt den Tokenstrom {
      \bi
      \ii \texttt{scanner\_next} konsumiert Zeichen vom Anfang bis zum Ende eines Tokens.
      \ii Implementierung: \alert<3>{manuell} oder mittels Regulären Ausdrücken und Tooling
      \ei
    }
    \ei
    \begin{code}<3->[tag=Py]
      \lstinputlisting[style=PY,linerange={scanner0-scanner1,scanner2-scanner3}]{lst/02-scanner.py}
    \end{code}
    \only<1|handout:0>{\begin{tikzpicture}[overlay,remember picture,box/.style={draw=srared,ultra thick}]
        \node[fit=(sig-name),box,pin=-100:Funktionsname]{};
        \node[fit=(sig-input),box,pin=-90:Parameter]{};
        \node[fit=(sig-output),box,pin=-85:Rückgabewert]{};
        \node[below=2 of sig-input] {\alert{Konvention}: Wir verwenden Funktionssignaturen aus Haskell};
      \end{tikzpicture}}%
  \end{frame}
#+end_src

Die Eingabe unseres Übersetzer ist eine Zeichenkette, die der
Programmier auf der Tastatur mühsam eingetippt und dann in einer Datei
gespeichert hat. In dem Prozess des kodierens hat der Programmierer
die Programmstruktur, die in seinem Geist enstanden ist, in diese Zeichenkette *serialisiert*.

Als ersten Schritt wollen wir die Zeichenkette in einen linearen
*Tokenstrom* aus sprachspezifischen Elementen, wie Schlüsselwörter und
Identifier, verwandeln. Dazu läuft der *Scanner* (= *Lexer*) einmal(!)
über die Zeichenkette und emitiert, immer wenn ein vollständiges
Sprachelement, was häufig aus mehr als einem Zeichen besteht, gelesen
wurde, ein einzelnes *Token*. Diese Token haben immer eine Tokenklasse
(z.B. Identifier oder "offnende runde Klammer") und manchmal eine
Nutzlast. In dieser Nutzlast transportiert der Scanner weitere
Informationen zum Token zu den Nachfolgenden Übersetzerstufen. So
haben beispielsweise alle Identifier die kleine Tokenklasse
(~identifier~) und tragen die überspannten Zeichen aus der Eingabe
(das Lexem) als Identifiername als Nutzlast mit sich herum.

Durch den Prozess des Scannens elimieren wir Leerzeichen und jegliche
Codeformatierung, wir vereinheitlichen unterschiedliche Schreibweisen
des gleichen Sprachelements und wir reduzieren die Anzahl der zu
verarbeitenden Objekte erheblich. Dies macht Übersetzerbauer das Leben
in folgenden erheblich einfacher, weil der Rest der Syntaxanalyse
deutlich schneller ablaufen kann und wir viele Sonderfälle der Sprache
bereits abgedeckt haben. So würden wir beispielsweise einen deutlich
aufwendigeren Parser benötigen, wenn wir erst dort Kommentare, die überall
im Quellcode auftauchen können, behandeln würden.

Um einen Scanner für eine Sprache zu erstellen haben wir prinzipiell
zwei Möglichkeiten: Entweder wir kodieren eine entsprechende Funktion
manuell oder wir leiten einen Scanner von einer Menge regulärer
Ausdrücke mit Hilfe eines Generators ab. Der manuelle Ansatz wird auch
*ad-hoc* Ansatz genannt, da die kodierung als ein normales Programm
keinem Formalismus folgt. Dies hat den Nachteil, dass es schwierig ist
einen korrekten Scanner mit der Hand zu schreiben, aber es bringt auch
den Vorteil, dass man an jeder Stelle Sonderbehandlungen einbauen
kann, mit sich.

In beiden Fällen benutzt der Scanner zwei Operationen auf dem
Eingabestrom: *read()* konsumiert ein Zeichen vom Anfang gibt es
zurück, *peek()* gibt dasselbe Zeichen zurück ohne es zu konsumieren.
Insbesondere letzteres, das Vorausschauen, oder *look-ahead*, im
Zeichenstrom ist ein Konzept, welches uns beim Parser wieder begegnen
wird. Eine wichtige Eigenschaft von Scannern und Parsern ist es wie
viele Zeichen der Vorausschau benötigt werden. Die meisten Sprachen
kommen mit einem Scanner aus, der nur ein einzelnes Zeichen Vorschau
benötigt[fn::Fortran ist ein trauriges Gegenbeispiel:
[[http://shape-of-code.coding-guidelines.com/2009/12/20/parsing-fortran-95/]]
].

Im Beispiel [fn::Vollständiger Scanner: [[file:./lst/02-scanner.py]]],
liefert die Funktion ~scanner_next()~ bei jedem Aufruf eine 2-Elementige
Liste, bestehend aus Tokentyp und Nutzlast, zurück. Die Funktion
überspringt zunächst alle Leerzeichen und Tabulatorzeichen, bevor die
eigentliche Tokenerkennung startet. Dieser Beispielscanner erkennt nur
Ganzzahlen, allerding ist es möglich diese Zahlen in binärer
(~0b111~), oktaler (~0777~), oder hexadezimaler Schreibweise zu
erkennen. Hierfür erkennt der Scanner, ob ein entsprechendes Präfix
vorliegt (z.B. ~0x~ für hexadezimal) und ließt dannach ensprechend der
erkannten Notation solange Zeichen aus der entsprechenden Klasse
zulässiger Zeichen (~stream.read_many()~).

#+INCLUDE: ../lst/02-scanner.py

An diesem Beispiel ist es wichtig zu erkennen, dass das Zeichen ~0~
ein *gemeinsames* Präfix ist und erst mit dem zweiten Zeichen (~b~,
~x~, oder eine Oktalzahl) wirklich feststeht welche Notation an dieser
Stelle vorliegt. Mit jedem weiteren gelesenen Zeichen, wird hier die
Menge der möglichen Notationen weiter eingeschränkt. So ist ab der
ersten gelesenen ~0~ klar, dass es sich nicht mehr um eine Zahl in
Dezimalnotation handeln kann.

#+begin_src latex
  \begin{frame}<handout:3>[t]{Wiederholung: Reguläre Sprachen und ihre Akzeptoren}
    \bi
    \ii Reguläre Sprachen werden von Typ-3 Grammatiken erzeugt. {
      \btDefTab[4cm]
      \bi
      \ii (Nicht-)Terminale: \btSetTab \emph{nonterminal} $\rightarrow$ X
      \ii Alternativen:      \btUseTab \emph{hexdigit} $\rightarrow$ 0 $\mid$ 1 $\mid$ 2  $\mid$ 3 $\mid$ \ldots  $\mid$ f
      \ii Konkatenierung:    \btUseTab \emph{hexprefix} $\rightarrow$ 0x
      \ii Wiederholung:      \btUseTab \emph{hexliteral} $\rightarrow$ \emph{hexprefix} \emph{hexdigit} \emph{hexdigit}*\hfill(Kleene Stern)
      \ei
    }\medskip
    \ii Reguläre Ausdrücke sind eine Kurzschreibweise für diese Grammatiken {
      \bi
      \ii Für hexadezimale Zahlen: 0x[0-9a0-f][0-9a-f]*
      \ii Verwendung zum Pattern Matching in Texten (\texttt{grep(1)}, Py: \texttt{import re}, \ldots)
      \ii Effizient Implementierbar in $\mathcal{O}(n)$, wenn $n$ die Eingabelänge
      \ei
    }
    \ii<2-> Darstellung und Implementierung: (nicht-)deterministische Automaten {\medskip
      \begin{center}
        \animation{1/2,2/3}{fig/02-automata-hex}
      \end{center}
      }
    \ei
  \end{frame}

  \begin{frame}<handout:1,2,3,7,9,11>{Scanner als Kombination von Regulären Automaten}
    \animation[width=\linewidth]{1/1,2/2,3/3,4/4,5/5,6/6,7/7,8/8,9/9,10/10,11/11,12/12}{fig/02-scanner}

    \bi
    \ii Automaten parallel über die Eingabe ausführen.{
      \bi
      \ii Longest Possible Match, Konfliktauflösung bei mehrfachem Match
      \ii Simulation des NFA, Umwandlung in \emph{determinstic finite automata} (\texttt{flex(1)})
      \ei
    }
    \ei
  \end{frame}

#+end_src

Für Schritt der Tokenerkennung aus einem Zeichenstrom hat
herausgestellt, dass der Rückgriff auf formale Konzepte aus der
theoretischen Informatik höchst lohnend ist und das man mit *regulären
Grammatiken* die Token der meisten Programmiersprachen hervorragend
beschreiben kann. Daher wollen wir nun der ad-hoc Implementierung ~scanner_next()~ aus dem Beispiel, einen formaleren Hintergrund geben.

Zu Beginn müssen wir uns daran erinnern, was Grammatiken überhaupt
waren. Die theoretische Informatikvorlesung ist lange her, und die
Erinnerung bereits verschommen. Zunächst ist eine (formale) Grammatik
ein Regelsatz, der beschreibt wie man, beginnend von einem
*Startsymbol*, über einen iterativen *Textersetzungsprozess*, zu einer
Zeichenkette kommt. Zu jedem Zeitpunkt besteht die Zeichenkette aus
einer Reihe von *terminalen* und *nicht-terminalen* Symbolen und die
Ersetzung wird solange wiederholt, bis nur noch terminale Symbole
übrig sind. Die Ersetzung an sich erfolgt durch eine Menge von Regel,
die ein Muster erkennt und durch ein anderes Muster ersetzt. Alle
Zeichenketten, die abgeleitet werden können nennt man die zugehörige
(formale) Sprache[fn::Verwechslungsgefahr: Formale Sprachen sind etwas
anderes als Programmiersprachen und sie haben nichts mit einem
Maschinenmodell zu tun. Das Skript unterscheidet daher strikt zwischen
formalen Sprachen und (einfach nur) Sprachen]. Im folgenden leiten
wir, Zeile für Zeile, die Zeichenkette ~0xa2c~ aus dem Startsymbol
~hexliteral~ her:

#+begin_example
hexliteral
hexprefix hexdigit hexdigit*
0x hexdigit hexdigit*
0xa hexdigit*
0xa2 hexdigit*
0xa2c hexdigit*
0xa2c
#+end_example

Je nachdem wie die Regeln einer Grammatik strukturiert sind, werden
Grammatiken in unterschiedliche Klassen eingeteilt
{{{wikipedia_de(Chomsky-Hierarchie)}}}. Eine der einfachsten Klassen, die
besonders wünschenswerte Eigenschaften hat, ist die Klasse der
*regulären Grammatiken*. Bei ihr gilt, dass auf der rechten Seite nur
ein einzelnes nicht-Terminal stehen darf. Dies bedeutet, dass die
Regel *kontextfrei* ist, da das nicht-Terminal, egal in welchem
Kontext es steht, immer gleich ersetzt wird. Weiterhin muss für
reguläre Grammatiken gelten, dass sie nicht rekursiv sind. Dies
bedeutet, dass man durch wiederholte Anwendung der Regel nie mehr zum
selben nicht-Terminal kommen darf. Beim Parsen werden wird
*kontextfreie* Grammatiken verwenden, die genau diese Einschränkung
nicht erfüllen und daher nicht regulär sind.

Das Angenehme an regulären Grammatiken ist, dass man sehr leicht ein
Programm ableiten kann, dass erkennt ob eine gegebene Zeichenkette von
der Grammatik erzeugt werden könnte. Wir drehen also die Richtung der
Grammatik um, erzeugen keine Zeichenketten mehr, sondern konsumieren
sie. Genau das, was wir für unseren Scanner brauchen.

Für das Erkennen einer regulären Sprache konstruiert man aus den
Grammatikregeln einen *endlichen Automaten* der die Zeichenkette
elementweise konsumiert. An jeder Kante des Automaten gibt es einen
Bedingung (engl. Guard), die angibt, bei welchen gelesenen Zeichen die
Transition möglich ist. Ist die Entscheidung, welche Transition
genommen wird, zu irgendeinem Zeitpunkt nicht eindeutig, so ist der
Automat *nicht-determinisitisch*. Erreichen wir beim Zeichen
konsumieren einen einen akzeptierenden Zustand, so haben wir ein Wort
der formalen Sprache erkannt. Allerdings können akzeptierende Zustände
auch wieder Verlassen um noch längere Worte der Sprache zu Erkennen.
Im Fall der Hexadezimalzahlen dürfen am Ende noch beliebig viele
Zeichen aus der Klasse ~[0-9a-fA-F]~ auftauchen.

Im Kontext eines Scanners bedeutet das Erreichen eines akzeptierenden
Zustandes, dass wir ein valides Token gefunden haben. Allerdings
könnte dieses Token auch das Prefix eines längeren Tokens sein. So ist
zwar ~0xa2~ eine valide hexadezimale Zahl, aber es ist auch das Prefix
für das Wort ~0xa2c~. Für Scanner wollen wir /immer/ jenes Token
finden, das maximale Länge hat (*longest possible match*).


Da es in einer Programmiersprache unterschiedliche Arten von Token
gibt, die es zu erkennen gilt, müssen wir mehrere Automaten
kombinieren; ein Automat/Grammatik/Sprache für jeden Tokentyp. Dazu
lassen wir "einfach" all diese Automaten parallel laufen und füttern
jedes gelesene Zeichen in alle Automaten gleichzeitig, sodass jeder
Automat eine Transition macht. Hat ein Automat zu einem Zeitpunkt
keine valide Transition, so fällt er aus (Folien: er wird grau) und
wird nicht weiter mit Zeichen gefüttert. Erreich einer der Automaten
einen akzeptierenden Zustand so merken wir uns das erkannte Wort (auch
Lexem genannt). Wir füttern die Automaten so lange mit Zeichen bis
auch der letzte von ihnen ausgefallen ist und geben das letzt
gefundene Wort als Token zurück.

Dieses parallel Ausführen von endlichen Automaten können wir entweder
tatsächlich durchführen oder können, was effizienter ist, einen
kombinerten Automaten konstruieren: Dazu fügt man einen weiteren
Startzustand ein, der über Epsilon-Transitionen die Startzustände der
einzelnen Token erreichen kann (was den kombierten Automaten immer
nicht-deterministisch macht). Mittels der [[https://de.wikipedia.org/wiki/Potenzmengenkonstruktion][Potenzmengenkonstruktion]]
leitet man den Equivalenten determinsitischen Automaten her.

Zurück zu den weniger theoretischen Aspekten des Scannens. Im Beispiel
auf den Folien, ist eine Situation abgebildet, bei der das
Schlüsselwort ~fn~ das Prefix für den Identifier ~fnord~ ist und zum
gleichen Zeitpunkt (~fn~) zwei Automaten das Lexem als valides Wort erkennen
würden. In solchen Fällen müssen wir zusätzliche Regeln angeben
welcher der Tokentypen den Vorrang hat. Für Programmiersprachen müssen
Schlüsselwörter immer hörere Priorität als Identifier haben, solange
der Identifier nicht länger ist. Dies ist der Grund, wieso Sie
Schlüsselwörter nicht als Variabelennamen verwenden können und diese
daher auch oft als *reservierte* Wörter genannt werden.

Da die manuelle Konstruktion von endlichen Automaten und deren
Kombination müßig ist, gibt es Programme, die aus einer Menge von
regulären Ausdrücken einen Scanner erzeugen. Ein solches Programm ist
[[https://www.cs.virginia.edu/~cr4bd/flex-manual/][flex(1)]]. Sollten sie später einen Lexer brauchen, der *schnell* und
*korrekt* eine Zeichenkette in Tokens zerlegt, so zögern sie nicht und
nehmen sie einen Scannergenerator zur Hand! Mehr zu den
Sicherheitsaspekten von Scannern, und auch Parsern, lernen wir Exkurs
am Ende dieser Vorlesung kennen.

* Parser und der Syntaxbaum
- Kontextfreie Grammatiken

** Parser mit Rekursivem Abstieg

** Die LL(1) Grammatiken

** Schwierigkeiten mit LL(1)
- Linksrekursion
- Common Prefixes
** LR(1) und Parsergeneratoren
- Dafür gibt es Lösungen, aber die Konstruktion der Parseautomaten ist schwieriger. Zum Glück gibt es dafür Tools.
* Exkurs: Parsing und Sicherheit
:PROPERTIES:
:CUSTOM_ID: 02-turing-trap
:END:

C++ Templates are Turin Complete: http://port70.net/~nsz/c/c%2B%2B/turing.pdf

https://stackoverflow.com/a/41418423/4099367
https://stackoverflow.com/questions/41415006/which-contemporary-computer-languages-are-ll1


* Zusammenfassung
#+begin_src latex
  \begin{frame}{Zusammenfassung}
  \end{frame}

  \begin{frame}{Quellenverzeichnis}
    \printbibliography
  \end{frame}
#+end_src
